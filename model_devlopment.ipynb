{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "transactions=data = pd.read_csv('normalise_train.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 0.102982\n",
      "Epoch 2/50, Loss: 0.017605\n",
      "Epoch 3/50, Loss: 0.009301\n",
      "Epoch 4/50, Loss: 0.005746\n",
      "Epoch 5/50, Loss: 0.003928\n",
      "Epoch 6/50, Loss: 0.003023\n",
      "Epoch 7/50, Loss: 0.002550\n",
      "Epoch 8/50, Loss: 0.002267\n",
      "Epoch 9/50, Loss: 0.002022\n",
      "Epoch 10/50, Loss: 0.001772\n",
      "Epoch 11/50, Loss: 0.001528\n",
      "Epoch 12/50, Loss: 0.001384\n",
      "Epoch 13/50, Loss: 0.001250\n",
      "Epoch 14/50, Loss: 0.001107\n",
      "Epoch 15/50, Loss: 0.000909\n",
      "Epoch 16/50, Loss: 0.000681\n",
      "Epoch 17/50, Loss: 0.000419\n",
      "Epoch 18/50, Loss: 0.000328\n",
      "Epoch 19/50, Loss: 0.000290\n",
      "Epoch 20/50, Loss: 0.000279\n",
      "Epoch 21/50, Loss: 0.000273\n",
      "Epoch 22/50, Loss: 0.000262\n",
      "Epoch 23/50, Loss: 0.000255\n",
      "Epoch 24/50, Loss: 0.000252\n",
      "Epoch 25/50, Loss: 0.000255\n",
      "Epoch 26/50, Loss: 0.000256\n",
      "Epoch 27/50, Loss: 0.000253\n",
      "Epoch 28/50, Loss: 0.000247\n",
      "Epoch 29/50, Loss: 0.000254\n",
      "Epoch 30/50, Loss: 0.000248\n",
      "Epoch 31/50, Loss: 0.000245\n",
      "Epoch 32/50, Loss: 0.000245\n",
      "Epoch 33/50, Loss: 0.000243\n",
      "Epoch 34/50, Loss: 0.000240\n",
      "Epoch 35/50, Loss: 0.000246\n",
      "Epoch 36/50, Loss: 0.000252\n",
      "Epoch 37/50, Loss: 0.000240\n",
      "Epoch 38/50, Loss: 0.000243\n",
      "Epoch 39/50, Loss: 0.000245\n",
      "Epoch 40/50, Loss: 0.000242\n",
      "Epoch 41/50, Loss: 0.000239\n",
      "Epoch 42/50, Loss: 0.000241\n",
      "Epoch 43/50, Loss: 0.000245\n",
      "Epoch 44/50, Loss: 0.000239\n",
      "Epoch 45/50, Loss: 0.000245\n",
      "Epoch 46/50, Loss: 0.000250\n",
      "Epoch 47/50, Loss: 0.000242\n",
      "Epoch 48/50, Loss: 0.000242\n",
      "Epoch 49/50, Loss: 0.000236\n",
      "Epoch 50/50, Loss: 0.000243\n",
      "Reconstruction error threshold: 0.000636\n",
      "Number of anomalies detected: 70\n",
      "Results saved to anomaly_detection_results.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_27308\\3582475710.py:131: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['Reconstruction_Error'] = full_errors\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_27308\\3582475710.py:132: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['Is_Anomalous'] = full_errors > threshold\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Step 1: Load and preprocess the dataset\n",
    "# Assume `transactions` DataFrame is loaded\n",
    "# Filter for relevant columns\n",
    "# Update feature columns to include new features\n",
    "selected_columns = [\n",
    "    'amount', 'transactions_per_day', 'mean_amount', 'std_amount', 'concentration_ratio', \n",
    "    'velocity_flag', 'amount_flag', 'time_flag', 'platform', 'product_category', 'payment_method'\n",
    "]\n",
    "\n",
    "# Update the DataFrame and preprocessing\n",
    "df = transactions[selected_columns]\n",
    "\n",
    "# One-hot encode categorical columns\n",
    "categorical_cols = ['platform', 'product_category', 'payment_method']\n",
    "df_encoded = pd.get_dummies(df, columns=categorical_cols)\n",
    "\n",
    "# Normalize numerical columns, including the new ones\n",
    "numerical_cols = ['amount', 'transactions_per_day', 'mean_amount', 'std_amount', 'concentration_ratio']\n",
    "scaler = MinMaxScaler()\n",
    "df_encoded[numerical_cols] = scaler.fit_transform(df_encoded[numerical_cols])\n",
    "\n",
    "# Proceed with the rest of the preprocessing and autoencoder training steps as before\n",
    "\n",
    "# Split into features (X) and train-test sets\n",
    "X = df_encoded.values\n",
    "\n",
    "# Filter normal data (label = 0 assumed)\n",
    "normal_data = transactions[transactions['label'] == 0]\n",
    "X_normal = df_encoded.iloc[normal_data.index].values\n",
    "\n",
    "# Train-test split\n",
    "train_data, test_data = train_test_split(X_normal, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 2: Define the Autoencoder model\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, latent_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, input_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "# Initialize the model\n",
    "input_dim = train_data.shape[1]\n",
    "latent_dim = 8  # Adjustable parameter\n",
    "model = Autoencoder(input_dim=input_dim, latent_dim=latent_dim)\n",
    "\n",
    "# Step 3: Train the Autoencoder\n",
    "# Convert data to PyTorch tensors\n",
    "train_data = train_data.astype(np.float32)\n",
    "\n",
    "train_tensor = torch.tensor(train_data, dtype=torch.float32)\n",
    "train_loader = DataLoader(TensorDataset(train_tensor), batch_size=32, shuffle=True)\n",
    "\n",
    "# Define training parameters\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "epochs = 50\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for batch in train_loader:\n",
    "        batch_data = batch[0]\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_data)\n",
    "        loss = criterion(outputs, batch_data)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {epoch_loss / len(train_loader):.6f}\")\n",
    "\n",
    "# Step 4: Calculate the reconstruction error threshold\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    reconstructions = model(train_tensor).numpy()\n",
    "    train_errors = np.mean((train_data - reconstructions) ** 2, axis=1)\n",
    "\n",
    "# Set the threshold (95th percentile of reconstruction errors)\n",
    "threshold = np.percentile(train_errors, 96)\n",
    "print(f\"Reconstruction error threshold: {threshold:.6f}\")\n",
    "\n",
    "# Step 5: Evaluate on test data\n",
    "# Convert test data to PyTorch tensors\n",
    "test_data = test_data.astype(np.float32)\n",
    "\n",
    "test_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_reconstructions = model(test_tensor).numpy()\n",
    "    test_errors = np.mean((test_data - test_reconstructions) ** 2, axis=1)\n",
    "\n",
    "# Classify anomalies\n",
    "anomalies = test_errors > threshold\n",
    "print(f\"Number of anomalies detected: {np.sum(anomalies)}\")\n",
    "\n",
    "# Step 6: Evaluate on full dataset\n",
    "# Prepare the full dataset (including normal and fraudulent data)\n",
    "\n",
    "X=X.astype(np.float32)\n",
    "full_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "\n",
    "with torch.no_grad():\n",
    "    full_reconstructions = model(full_tensor).numpy()\n",
    "    full_errors = np.mean((X - full_reconstructions) ** 2, axis=1)\n",
    "\n",
    "# Add anomaly scores to the original dataset\n",
    "df['Reconstruction_Error'] = full_errors\n",
    "df['Is_Anomalous'] = full_errors > threshold\n",
    "\n",
    "# Save results to a CSV\n",
    "df.to_csv(\"anomaly_detection_results.csv\", index=False)\n",
    "print(\"Results saved to anomaly_detection_results.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
